{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import mujoco\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n",
      "WARNING: Geom with duplicate name '' encountered in URDF, creating an unnamed geom.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class QuadExEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 60}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the URDF model\n",
    "        model_path = \"tesbot.urdf\"\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Missing URDF: {model_path}\")\n",
    "\n",
    "        # Convert URDF to MJCF model\n",
    "        self.model = mujoco.MjModel.from_xml_path(model_path)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "\n",
    "        # Action and observation space (8 joints)\n",
    "        self.n_actuators = self.model.nu\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(self.n_actuators,), dtype=np.float32)\n",
    "\n",
    "        obs_size = self.model.nq + self.model.nv\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_size,), dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.viewer = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset state\n",
    "        self.data.qpos[:] = 0\n",
    "        self.data.qvel[:] = 0\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Clip to action limits\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        self.data.ctrl[:] = action\n",
    "\n",
    "        mujoco.mj_step(self.model, self.data)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        # Basic reward for staying upright\n",
    "        reward = 1.0 if self.data.qpos[2] > 0.05 else 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate([self.data.qpos, self.data.qvel]).copy()\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            if self.viewer is None:\n",
    "                self.viewer = mujoco.viewer.launch_passive(self.model, self.data)\n",
    "            self.viewer.sync()\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n",
    "env = QuadExEnv(render_mode=\"human\")\n",
    "env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.position = 0\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class QuadExAgent():\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: QuadExEnv,\n",
    "            learning_rate: float,\n",
    "            hidden_dim: int,\n",
    "            batch_size: int,\n",
    "\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create the neural network model\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, env.action_space.shape[0]),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # Replay buffer\n",
    "        self.buffer = []\n",
    "    def run(self, is_training: bool, num_episodes: int, render: bool = False):\n",
    "        env = QuadExEnv(render_mode=\"human\") if render else QuadExEnv()\n",
    "        policy_dqn = DQN(env.observation_space.shape[0], env.action_space.shape[0], self.hidden_dim).to(device)\n",
    "\n",
    "        if is_training:\n",
    "            memory = ReplayMemory(10000)\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            new_state, reward, terminated, _, info = env.step(action)\n",
    "            if is_training:\n",
    "                memory.append((new_state, action, reward, terminated))\n",
    "                \n",
    "            if terminated:\n",
    "                break\n",
    "        env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
